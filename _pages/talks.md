---
layout: archive
title: "Talks and presentations"
permalink: /talks/
author_profile: true
---

Various talks or presentations I've given at conferences, workshops, or seminars.

---

<!-- Going to do this in pure markdown, so comment out the boilerplate>

<!-- {% if site.talkmap_link == true %}

<p style= "text-decoration:underline;"><a href="/talkmap.html">See a map of all the places I've given a talk!</a></p>

{% endif %}

{% for post in site.talks reversed %}
  {% include archive-single-talk.html %}
{% endfor %} -->

‚Ü™ __Generate, don't discriminate! Modelling biomedical information extraction tasks with sequence-to-sequence learning__. Invited talk at the [Data Science & Artificial Intelligence group at AstraZeneca](https://www.astrazeneca.com/r-d/data-science-and-ai.html) (_August 2022_). I spoke about [our recent work](https://aclanthology.org/2022.bionlp-1.2/) on using sequence-to-sequence models for end-to-end biomedical information extraction tasks [üõù [slides](https://docs.google.com/presentation/d/1eODyc1zvqGwP0iiwVeszII0kskGklxZsA_ogekRYabY/edit?usp=sharing)]

‚Ü™ __Natural Language Processing and Transformers__. I created and delivered a tutorial for the University of Toronto's [CSC413 Neural Networks and Deep Learning](https://uoft-csc413.github.io/2023/) class (_Winter 2022 & 2023_). This tutorial focused on helping students build intuition about the [Transformer architecture](https://arxiv.org/abs/1706.03762), and specifically the self-attention mechanism [üõù [slides](https://docs.google.com/presentation/d/1GlOEERsbu71LhpT1L-nMcDtOAwgg8DPScbNkjF989zc/edit?usp=sharing)]